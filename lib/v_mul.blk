-- 
-- Copyright (c) Microsoft Corporation
-- All rights reserved. 
--
-- Licensed under the Apache License, Version 2.0 (the ""License""); you
-- may not use this file except in compliance with the License. You may
-- obtain a copy of the License at
--
-- http://www.apache.org/licenses/LICENSE-2.0
--
-- THIS CODE IS PROVIDED ON AN *AS IS* BASIS, WITHOUT WARRANTIES OR
-- CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT
-- LIMITATION ANY IMPLIED WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR
-- A PARTICULAR PURPOSE, MERCHANTABLITY OR NON-INFRINGEMENT.
--
-- See the Apache Version 2.0 License for specific language governing
-- permissions and limitations under the License.
--
--
#pragma once 

-- a * b
-- re32 = a.re * b.re - a.im * b.im
-- im32 = a.im * b.re + a.re * b.im
-- 
-- Note: we don't pack it in complex32 
-- as it requires extra cycles and is often not needed
fun v_mul_complex16_int32(var re:arr int, var im:arr int, a:arr complex16, b:arr complex16) {
  var vs1:arr[4] complex16;
  var vs2:arr[4] complex16;

  let vlen = (length(a) / 4) 
  let rlen = (length(a) - vlen*4)  

  for i in [0, vlen] {
    -- Return by reference for performance
    --vs1 := conj0w(b[4*i,4]);
    conj0w(vs1, b[4*i,4]);
    permutate_low1032w(b[4*i,4], vs2);
    permutate_high1032w (vs2,vs2);
    -- Return by reference for performance
    --re[4*i,4] := muladdw(a[4*i,4],vs1);
    --im[4*i,4] := muladdw(a[4*i,4],vs2)
    muladdw(re[4*i,4], a[4*i,4],vs1);
    muladdw(im[4*i,4], a[4*i,4],vs2)
  }

  -- Sum the rest one by one, if length is not modulo 4
  for i in [vlen*4,rlen] {
    re[i] := (int(a[i].re)*int(b[i].re) - int(a[i].im)*int(b[i].im ));
    im[i] := (int(a[i].im)*int(b[i].re) + int(a[i].re)*int(b[i].im ));
  };
}



-- This was slow because of multiple consecutive loops so we write a specialized
-- version below
{-
-- c = (a * b) << shift
-- We need to do shift before casting to complex16 to avoid loss of precision
fun v_mul_complex16_slow(c:arr complex16, a:arr complex16, b:arr complex16, shift:int) { 
  var re32 : arr[length(a)] int32;
  var im32 : arr[length(a)] int32;

  v_mul_complex16_int32(re32, im32, a, b);
  if (shift > 0) then {
    v_shift_right_int32(re32,re32,shift);
    v_shift_right_int32(im32,im32,shift);
  }
  v_pack_int32_complex16(c,re32,im32);
} in
-}



-- c = (a * b) << shift
-- We need to do shift before casting to complex16 to avoid loss of precision
fun v_mul_complex16_shift(var c:arr complex16, a:arr complex16, b:arr complex16, shift:int) { 
  var vs1:arr[4] complex16;
  var vs2:arr[4] complex16;
  var re32 : arr[4] int32;
  var im32 : arr[4] int32;

  let vlen = (length(a) / 4) 
  let rlen = (length(a) - vlen*4)  

  for i in [0, vlen] {
    -- Return by reference for performance
    --vs1 := conj0w(b[4*i,4]);
    conj0w(vs1, b[4*i,4]);
    permutate_low1032w(b[4*i,4], vs2);
    permutate_high1032w (vs2,vs2);
    -- Return by reference for performance
    --re[4*i,4] := muladdw(a[4*i,4],vs1);
    --im[4*i,4] := muladdw(a[4*i,4],vs2)
    muladdw(re32, a[4*i,4],vs1);
    muladdw(im32, a[4*i,4],vs2);
    v_shift_right_int32(re32,re32,shift);
    v_shift_right_int32(im32,im32,shift);
    v_pack_int32_complex16(c[4*i,4],re32,im32);
  }


  -- Sum the rest one by one, if length is not modulo 4
  for i in [vlen*4,rlen] {
    re32[i-vlen*4] := (int(a[i].re)*int(b[i].re) - int(a[i].im)*int(b[i].im )) >> shift;
    im32[i-vlen*4] := (int(a[i].im)*int(b[i].re) + int(a[i].re)*int(b[i].im )) >> shift;
    c[i] := complex16{re=int16(re32[i-vlen*4]); im=int16(im32[i-vlen*4])};
  };

}
